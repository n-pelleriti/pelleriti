---
title: "Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms"
collection: publications
category: conferences
permalink: /publication/2025-12-01-computational-algebra-attention
excerpt: ''
date: 2025-05-23
venue: 'Conference on Neural Information Processing Systems (NeurIPS)'
authors: 'Hiroshi Kera*, Nico Pelleriti*, Yuki Ishihara, Max Zimmer, Sebastian Pokutta'
paperurl: 'https://arxiv.org/abs/2505.23696'
codeurl: 'https://github.com/ZIB-IOL/OracleBorderBasis'
image: '/images/borderbasis.png'
citation: 'Kera, H.*, Pelleriti, N.*, Ishihara, Y., Zimmer, M., & Pokutta, S. (2025). Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms. <i>Conference on Neural Information Processing Systems (NeurIPS)</i>. San Diego, CA. (*equal contribution)'
bibtex: "@inproceedings{kera2025computational,\n  title={Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms},\n  author={Kera, Hiroshi and Pelleriti, Nico and Ishihara, Yuki and Zimmer, Max and Pokutta, Sebastian},\n  booktitle={Conference on Neural Information Processing Systems (NeurIPS)},\n  year={2025},\n  note={Equal contribution: Kera and Pelleriti}\n}"
---

We introduce a learning-augmented approach to computational algebra, where transformer models are trained to serve as oracles for a classical symbolic algorithm. Specifically, we focus on border basis algorithms, demonstrating significant speedups.

