---
title: 'Neural Sum-of-Squares: Certifying Polynomial Nonnegativity with Transformers'
date: 2025-12-01
permalink: /posts/2025/12/neural-sum-of-squares/
tags:
  - Transformer
  - Polynomial Optimization
  - Sum of Squares
  - Semidefinite Programming
---

*This post accompanies our recent Preprint: [Neural Sum-of-Squares: Certifying the Nonnegativity of Polynomials with Transformers](https://github.com/ZIB-IOL/Neural-Sum-of-Squares)*

Many problems across control theory and robotics can be formulated as NP-hard polynomial optimization programs. 
A common relaxation is the *Sum-of-Squares (SOS)* formulation, which is a programming technique that reduces the problem to solving a *Semidefinite Program (SDP)*. 
However, the size of these SDPs grows rapidly with the polynomial's complexity, making even moderately sized problems computationally prohibitive. 
In this work, we develop a learning-augmented algorithm that uses a Transformer to predict compact monomial bases, yielding significant computational savings while preserving correctness guarantees.

---

## The Nonnegativity Certification Problem

Optimizing the value of a polynomial $p(x)$ can equivalently be formulated as proving nonnegativity: What is the largest parameter $\lambda$, such that $p(x) - \lambda$ is nonnegative over its entire domain?

For illustration, fix $\lambda$ and consider the simple example:

$$p(x_1, x_2) = 4x_1^4 + 12x_1^2x_2^2 + 9x_2^4 + 1$$

Is this polynomial always nonnegative? One way to verify this is to write it as a **Sum-of-Squares (SOS)**:

$$p(x_1, x_2) = (2x_1^2 + 3x_2^2)^2 + 1^2$$

Since any sum of squares is clearly nonnegative, this *SOS decomposition* serves as a certificate of nonnegativity. The key insight from [1] is that checking whether such a decomposition exists can be formulated as a _Semidefinite Program_. 

## From SOS to Semidefinite Programming

A polynomial $p(\mathbf{x})$ is SOS if and only if there exists a positive semidefinite matrix $Q$ such that:

$$p(\mathbf{x}) = \mathbf{z}(\mathbf{x})^\top Q \, \mathbf{z}(\mathbf{x})$$

where $\mathbf{z}(\mathbf{x})$ is a vector of monomials (the *basis*). For our example, the hand-written decomposition $(2x_1^2 + 3x_2^2)^2 + 1^2$ corresponds to choosing $\mathbf{z}(\mathbf{x}) = [1, x_1^2, x_2^2]^\top$, where expanding the quadratic form yields the original polynomial.

In general, the standard approach uses all monomials up to half the polynomial's degree:

$$\mathbf{z}(\mathbf{x}) = [1, x_1, x_2, x_1x_2, x_1^2, x_2^2]^\top$$

This gives a 6×6 matrix $Q$ to optimize over. However, the same SOS decomposition can be computed using a much smaller basis:

$$\mathbf{z}'(\mathbf{x}) = [1, x_1^2, x_2^2]^\top$$

with the 3×3 matrix:

$$Q = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 4 & 6 \\ 0 & 6 & 9 \end{pmatrix}$$

The computational cost of solving an SDP scales polynomially in the matrix dimension—so using a basis of size 3 instead of 6 yields substantial savings. 
For larger problems with dozens of variables and high degrees, the size of the basis often determines if the problem is tractable at all. 

## The Basis Selection Bottleneck

A central computational challenge in SOS programming is **basis selection**: finding a compact set of monomials that admits an SOS decomposition while keeping the SDP as small as possible.

<figure class="fig-side-by-side fig-white-bg" style="max-width: 80%;">
  <div class="subfig">
    <div class="sublabel">(a)</div>
    <img src="/images/blogpost_figures/newton_polytope-1.png" alt="Newton Polytope">
  </div>
  <div class="subfig">
    <div class="sublabel">(b)</div>
    <img src="/images/blogpost_figures/half_newton_polytope-1.png" alt="Half Newton Polytope">
  </div>
  <figcaption>(a) The Newton polytope of a polynomial captures geometric constraints on which monomials can appear in an SOS basis. (b) The half Newton polytope defines the candidate set for basis monomials.</figcaption>
</figure>

A classical result [2] constrains which monomials can appear in a valid basis: they must lie within the *half Newton polytope* $\frac{1}{2}N(p)$. The Newton polytope is the convex hull of a polynomial's exponent vectors; the half Newton polytope scales this by half to define the candidate set for basis monomials. This provides an upper bound on the candidate set, but this set is still typically much larger than necessary.

Traditional methods like Newton polytope with diagonal consistency, chordal sparsity exploitation, and TSSOS [4] provide rule-based approaches to reducing basis size. 
However, they often produce bases significantly larger than the actual minimal basis.


## Method: Learning to Predict Compact Bases

We frame basis selection as a sequence prediction problem and train a Transformer to solve it. Given a polynomial $p(\mathbf{x})$, the model predicts a set of monomials likely to form a valid SOS basis.

<figure class="fig-white-bg">
  <img src="/images/blogpost_figures/schematic_large-1.png" alt="Schematic Overview">
  <figcaption>Overview of our learning-augmented SOS framework: (i) A Transformer predicts a compact basis from the polynomial, (ii) the basis is adjusted to satisfy necessary conditions, and (iii) an SDP is solved with iterative expansion if needed.</figcaption>
</figure>

Our approach operates in three stages:

### Stage 1: Transformer-Based Basis Prediction

We tokenize polynomials using a monomial-level embedding scheme from [3]. Each term is represented as a sequence of tokens encoding the coefficient and exponents. For example, the term $4x_1^4$ becomes "C4.0 E4 E0" (coefficient 4.0, $x_1$ to the 4th power, $x_2$ to the 0th power), while $12x_1^2x_2^2$ becomes "C12.0 E2 E2". The model generates basis monomials autoregressively until producing an end-of-sequence token.

Training data is generated through **reverse sampling**: we sample a monomial basis $B$, draw a random PSD matrix $Q$, and compute $p(\mathbf{x}) = \mathbf{z}_B(\mathbf{x})^\top Q \, \mathbf{z}_B(\mathbf{x})$. This guarantees $B$ is a valid SOS basis by construction and typically produces near-minimal bases.

<figure class="fig-50 fig-white-bg">
  <img src="/images/blogpost_figures/matrix_structures-1.png" alt="Matrix Structures">
  <figcaption>We train on diverse polynomial families generated from different PSD matrix structures: dense full-rank, sparse, block diagonal, and low-rank.</figcaption>
</figure>

### Stage 2: Coverage Repair

A predicted basis must satisfy a necessary condition: every monomial in the polynomial's support must be expressible as a product of two basis monomials. For example, the set $\\{x_1^2, x_1x_2, 1\\}$ cannot be a basis for $p(x_1, x_2) = 4x_1^4 + 12x_1^2x_2^2 + 9x_2^4 + 1$, since none of the products of these monomials forms $x_2^4$, which appears in the support of $p$.

If the predicted basis violates this condition, we apply a greedy repair algorithm that iteratively adds monomials covering the most missing support terms.

### Stage 3: Iterative Expansion with Verification

If the coverage-repaired basis still fails to yield a feasible SDP, we expand it systematically. Rather than adding monomials arbitrarily, we rank candidates using a **permutation-based scoring** mechanism:

We run the Transformer on multiple random permutations of the polynomial's variables, then score each monomial by how frequently it appears across predictions. Monomials with higher scores are added first during expansion.


## Empirical Evaluation

Finally, we evaluate our method on a total of over 200 million polynomials, comparing against a variety of benchmarks. 
The results depend on structure of the polynomials—and for favorable structures, we observe substantial computational savings.

### Large-Scale Performance

We first demonstrate performance on challenging large-scale configurations, comparing against state-of-the-art SOS solvers: SoS.jl [6] (standard Newton polytope), TSSOS [4] (exploiting term sparsity), and Chordal-TSSOS [5] (exploiting chordal sparsity).

| Configuration | Ours | SoS.jl | TSSOS | Chordal-TSSOS |
|:-------------|:----:|:------:|:-----:|:-------:|
| 6 vars, deg 20 | 5.64s | 119.98s | 86.53s | 105.54s |
| 6 vars, deg 40 | 42.8s | -- | -- | -- |
| 8 vars, deg 20 | 1.46s | 3037.85s | 2674.50s | 3452.98s |
| 100 vars, deg 10 | 18.3s | -- | -- | -- |

*Table shows average solve time in seconds. "--" indicates out-of-memory or timeout.*

Our method demonstrates strong scalability advantages. For 8 variables at degree 20, we achieve over 1800× speedup compared to standard approaches. More significantly, our method successfully solves problems where all baselines fail due to memory constraints—including 6 variables at degree 40 and the extreme case of 100 variables at degree 10, which remains intractable for traditional methods.

### Results on Sparse Polynomials

We benchmark against the standard Newton polytope method across polynomials with different underlying structures. The table below shows results for polynomials that were sampled using a sparse matrix $Q$. 

| Variables | Degree | Basis Size (Ours) | Basis Size (Newton) | Time (Ours) | Time (Newton) | Speedup |
|:---------:|:------:|:-----------------:|:-------------------:|:-----------:|:-------------:|:-------:|
| 4 | 6 | 15 | 18 | 0.23s | 0.20s | 0.9× |
| 6 | 12 | 27 | 40 | 0.57s | 1.20s | 2.1× |
| 8 | 20 | 27 | 28 | 0.62s | 15.3s | 25× |
| 6 | 20 | 73 | 233 | 7.4s | 1606s | 217× |

On small problems, the overhead of running a Transformer offsets any gains from reduced basis size. 
However, as problems scale, the difference in basis size translates to substantial savings in SDP solve time. 
For the 6-variable, degree-20 case, the predicted basis (73 monomials) is approximately 3× smaller than the Newton polytope basis (233 monomials), reducing solve time from 1606 seconds to 7.4 seconds.

### Dependence on Polynomial Structure

The Transformer does not succeed uniformly. 
Its raw prediction accuracy—before any repair—varies with the polynomial's underlying structure.

<figure class="">
  <img src="/images/blogpost_figures/heatmaps-1.png" alt="Heatmaps">
  <figcaption>Failure rate heatmaps across polynomial degree and structure. Darker cells indicate higher failure rates.</figcaption>
</figure>

These heatmaps illustrate that polynomial structure significantly affects prediction difficulty. 
Sparse and block-diagonal polynomials exhibit regularities in their coefficient matrices that the Transformer learns to exploit, achieving 85–95% success rates. 
Dense and low-rank cases lack such structure, making the prediction task inherently harder.

This behavior is consistent with expectations: basis selection is easier when the underlying problem has exploitable structure. 
Notably, many practical SOS applications (control Lyapunov functions, sparse polynomial optimization) fall into the favorable categories.

### Repair Mechanisms

Raw prediction accuracy alone does not determine algorithm performance. When the Transformer's initial basis is insufficient, the repair mechanisms attempt to recover a valid basis.

<figure class="fig-75">
  <img src="/images/blogpost_figures/repair-1.png" alt="Repair Outcomes">
  <figcaption>Repair outcomes by polynomial structure. "Insufficient" means the final basis still failed after all repairs.</figcaption>
</figure>

This figure shows what happens after repair. 
The greedy coverage repair handles most failures cheaply—it simply adds missing monomials to satisfy necessary conditions. 
The permutation-based expansion handles the harder cases where the SDP itself fails, using ensemble predictions to guide monomial selection.

The combined approach reduces insufficient cases to under 5% across all structures. Even when repair is needed, the final basis typically remains compact enough to preserve computational advantages over baselines.

### Runtime Analysis

A relevant consideration is whether neural network inference adds substantial overhead. We analyze the runtime breakdown to understand the computational costs.

<figure class="fig-75">
  <img src="/images/blogpost_figures/time_breakdown_normalized-1.png" alt="Time Breakdown">
  <figcaption>Runtime breakdown by problem size. Each bar shows the fraction of time spent on Transformer inference, repair, and SDP solving.</figcaption>
</figure>

The results show that SDP solving dominates the runtime. For larger problems, the SDP accounts for 75–80% of total runtime, while Transformer inference and repair overhead are comparatively small. This supports the premise that reducing basis size is the primary factor affecting overall performance, and the cost of prediction is acceptable.


## Limitations

**Synthetic benchmarks.** The evaluation relies on synthetic polynomial generation. Access to real-world SOS problem instances would provide additional empirical validation.

**Non-SOS polynomials.** For polynomials that are not SOS, the method cannot accelerate the computation—proving non-existence requires considering the full Newton polytope. The overhead ranges from 2.2× for small problems to 1.05× for larger instances.

## Conclusion

This work presents a learning-augmented approach to SOS programming that addresses the computational bottleneck of basis selection while maintaining correctness guarantees.

The main contributions are:

1. A learning-augmented algorithm for SOS programming with theoretical consistency and robustness guarantees
2. Empirical speedups of up to two orders of magnitude on structured polynomial instances
3. Repair mechanisms that recover valid bases when initial predictions are insufficient
4. Scalability to 100 variables, extending the practical reach of SOS methods

This approach suggests potential for integrating machine learning with formal verification methods in optimization and control.


```bibtex
@misc{pelleriti2025neuralsumofsquarescertifyingnonnegativity,
      title={Neural Sum-of-Squares: Certifying the Nonnegativity of Polynomials with Transformers}, 
      author={Nico Pelleriti and Christoph Spiegel and Shiwei Liu and David Martínez-Rubio and Max Zimmer and Sebastian Pokutta},
      year={2025},
      eprint={2510.13444},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2510.13444}, 
}
```

## References 

[1] Parrilo, P. A. (2003). Semidefinite programming relaxations for semialgebraic problems. *Mathematical Programming*, 96(2), 293–320.

[2] Reznick, B. (1978). Extremal PSD forms with few terms. *Duke Mathematical Journal*, 45(2), 363–374.

[3] Kera, H., Pelleriti, N., Ishihara, Y., Zimmer, M., & Pokutta, S. (2025). Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms. *arXiv preprint arXiv:2505.23696*.

[4] Wang, J., Magron, V., & Lasserre, J. B. (2019). TSSOS: A Moment-SOS Hierarchy That Exploits Term Sparsity. *SIAM Journal on Optimization*, 31, 30–58.

[5] Wang, J., Magron, V., & Lasserre, J. B. (2020). Chordal-TSSOS: A Moment-SOS Hierarchy That Exploits Term Sparsity with Chordal Extension. *SIAM Journal on Optimization*, 31, 114–141.

[6] Weisser, T., Legat, B., Coey, C., Kapelevich, L., & Vielma, J. P. (2019). Polynomial and Moment Optimization in Julia and JuMP. *JuliaCon 2019*.






